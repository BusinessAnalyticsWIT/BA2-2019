<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.2.10/semantic.min.css"
          type="text/css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/railscasts.min.css"
          rel="stylesheet"/>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/jquery.address/1.6/jquery.address.min.js"></script>
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.2.10/semantic.min.js"></script>
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <style>
      

body {
  font-family: "Open Sans", "Helvetica", "Helvetica Neue",  "Arial", sans-serif;
  font-size:90%;
  color: black;
}

p {
  margin: 0.5em;
}

pre code {
  font-family: "Monaco";
  font-size: 100%;
}

img {
  box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
  margin:10px;
}

h1, h2, h3 {
  border-bottom:thin solid black;
  margin-bottom: 0.5em;
  margin-top: 1em;
}

h1 {
  font-style:italic;
  font-size:130%;
}

h2 {
  font-size:110%;
}

h3 {
  font-size:100%;
}



      

html, body {
  height: 100%;
  margin: 0;
}
.wrapper {
  height: 100%;
  display: flex;
  flex-direction: column;
}
.footer {
  background: white;
  text-align: right;
}
.content {
  flex: 1;
  overflow: auto;
}



    </style>
  </head>

  
  

  <div class="ui fixed top pointing inverted stackable menu labmenu">
    <header class="header item">
      <i id="toc" class="sitemap icon"></i>
      
        <a href="../index.html"> Machine Learning  </a>
      
    </header>
    <div class="right tab-menu menu">
      
        <a class="item" data-tab="Lab-09">
          Lab-09
        </a>
      
        <a class="item" data-tab="01">
          01
        </a>
      
        <a class="item" data-tab="02">
          02
        </a>
      
        <a class="item" data-tab="03">
          03
        </a>
      
    </div>
  </div>


  

  <div class="ui pushable">
    <div class="ui inverted labeled icon left inline vertical sidebar menu">
      <br><br>
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-01-Python-Data-Exploration/book-a/index.html">Lab-01 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-02-Data-Management/book/index.html">Lab-02 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-02-Data-Management/book-a/index.html">Lab-03 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-03-Visualisation/book/index.html">Lab-04 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-03-Visualisation/book-a/index.html">Lab-05 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-04-Hypothesis-Testing/book/index.html">Lab-06 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-04-Hypothesis-Testing/book-ca/index.html">Lab-07 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-05-Regression-Clustering/book/index.html">Lab-08 </a>
          
        
      
        
          
            <a class="item"
               href="http://businessanalyticswit.github.io/BA2-2019///topic-05-Regression-Clustering/book-a/index.html">Lab-09 </a>
          
        
      
    </div>
    <div class="pusher" tabindex="-1">
      <div class="ui basic segment">
        <br>
        
          <div class="ui tab segment lab" data-tab="Lab-09">
            <h1>Cluster Analysis</h1>
<p>Continuous Assessment conducting cluster analysis</p>

          </div>
        
          <div class="ui tab segment lab" data-tab="01">
            <h1>Cluster analysis</h1>
<p>Create a new file called Lab09_clustering.py</p>
<p>To run a K-means cluster analysis in Python, first we call in the libraries we need.</p>
<p>In addition to pandas, numpy and matplotlib libraries we&#39;ll need the train_test_split function from the sklearn.cross_validation library, and the pre processing function from the sklearn library.
Finally we need the k-Means function fro sklearn.cluster library.</p>
<pre><code>
from pandas import Series, DataFrame
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.cluster import KMeans</code></pre>
<p>Save the following csv file to your working directory</p>
<ul>
<li><a href="./archives/lab11_addhealth.csv">add health csv file</a></li>
</ul>
<p>We will load this file into our dataframe and conduct some basic data management.</p>
<pre><code>
&quot;&quot;&quot;
Data Management
&quot;&quot;&quot;
data = pd.read_csv(&quot;lab11_addhealth.csv&quot;)

#upper-case all DataFrame column names
data.columns = map(str.upper, data.columns)

# Data Management

data_clean = data.dropna()

# subset clustering variables
cluster=data_clean[[&#39;ALCEVR1&#39;,&#39;MAREVER1&#39;,&#39;ALCPROBS1&#39;,&#39;DEVIANT1&#39;,&#39;VIOL1&#39;,
&#39;DEP1&#39;,&#39;ESTEEM1&#39;,&#39;SCHCONN1&#39;,&#39;PARACTV&#39;, &#39;PARPRES&#39;,&#39;FAMCONCT&#39;]]
cluster.describe()</code></pre>
<p>Once the data is loaded we convert all column names to uppercase, and then clean out all observations that have missing data on any variables using the dropna function. We then create a data set called cluster which only contains the variables to used for clustering.</p>
<p>In cluster analysis variables with large values contribute more to the distance calculations. Variables measured on different scales should be standardized prior to clustering, so that the solution is not driven by variables measured on larger scales. We use the following code to standardize the clustering variables to have a mean of 0, and a standard deviation of 1. This ensures all variables contribute equally to a scale when items are added together.</p>
<pre><code># standardize clustering variables to have mean=0 and sd=1
clustervar=cluster.copy()
clustervar[&#39;ALCEVR1&#39;]=preprocessing.scale(clustervar[&#39;ALCEVR1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;ALCPROBS1&#39;]=preprocessing.scale(clustervar[&#39;ALCPROBS1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;MAREVER1&#39;]=preprocessing.scale(clustervar[&#39;MAREVER1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;DEP1&#39;]=preprocessing.scale(clustervar[&#39;DEP1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;ESTEEM1&#39;]=preprocessing.scale(clustervar[&#39;ESTEEM1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;VIOL1&#39;]=preprocessing.scale(clustervar[&#39;VIOL1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;DEVIANT1&#39;]=preprocessing.scale(clustervar[&#39;DEVIANT1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;FAMCONCT&#39;]=preprocessing.scale(clustervar[&#39;FAMCONCT&#39;].astype(&#39;float64&#39;))
clustervar[&#39;SCHCONN1&#39;]=preprocessing.scale(clustervar[&#39;SCHCONN1&#39;].astype(&#39;float64&#39;))
clustervar[&#39;PARACTV&#39;]=preprocessing.scale(clustervar[&#39;PARACTV&#39;].astype(&#39;float64&#39;))
clustervar[&#39;PARPRES&#39;]=preprocessing.scale(clustervar[&#39;PARPRES&#39;].astype(&#39;float64&#39;))


# split data into train and test sets
clus_train, clus_test = train_test_split(clustervar, test_size=.3, random_state=123)</code></pre>
<p>To standardize the clustering variables we will first create a copy of the cluster data frame and name it clustervar, then we use the preprocessing.scale function to transform the clustering variables to have a mean of 0 and a standard deviation of 1. Astype float64 ensures that my clustering variables have a numeric format, and we will do this for all the clustering variables.</p>
<p>The second part of the code is where we split the data into train and test sets. The split function in the sklearn cross validation library randomly splits the clustering variable data set into a training data set consisting of 70% of the total observations, and a test data set consisting of the other 30% of the observations.</p>
<p>The test_size option tells Python to randomly place 0.3, that is 30% of the observations in the test data set that we named clus_test. By default the other 70% of the observations are placed in the clus_train training dataset. The random_state option specifies a random number seat to ensure that the data are randomly split the same way if I run the code again.</p>
<p>Now we are ready to run our cluster analysis, because we don&#39;t know how many clusters actually exist in the population for a range of values on the number of clusters before we begin we&#39;ll import the cdist function from the scipy.spatial.distance library. In this example we will use it to calculate the average distance of the observations from the cluster centroids. Later, we can plot this average distance measure to help us figure out how many clusters may be optimal, then we will create an object called clusters that will include numbers in the range between 1 and 10. We will use this object when we specify the number of clusters we want to test, which will give us the cluster solutions for k equals 1 to k equals 9 clusters. In the last line of code we create an object called meandist that will be used to store the average distance values that we will calculate for the 1 to 9 cluster solutions.</p>
<pre><code># k-means cluster analysis for 1-9 clusters
from scipy.spatial.distance import cdist
clusters=range(1,10)
meandist=[]</code></pre>
<p>The next piece of code runs the cluster analysis.</p>
<pre><code>
for k in clusters:
    model=KMeans(n_clusters=k)
    model.fit(clus_train)
    clusassign=model.predict(clus_train)
    meandist.append(sum(np.min(cdist(clus_train, model.cluster_centers_, &#39;euclidean&#39;), axis=1))
    / clus_train.shape[0])</code></pre>
<p>This code tells Python to run the cluster analysis code for each value of k in the cluster&#39;s object. Then the k-Means function from the ski learning cluster library to run the cluster analysis. In parentheses n_clusters indicates the number of clusters, which in our example we substitute with k to tell Python to run the cluster analysis for 1 through 9 clusters.</p>
<p>Then we create an object called clusassign that will store for each observation the cluster number to which it was assigned based on the cluster analysis.
Model.predict asks that the results of the cluster analysis stored in the model object be used to predict the closest cluster that each observation belongs to.</p>
<p>Then we calculate an average distance measure for each cluster solution. The code following meandist.append computes the average of the sum of the distances between each observation in the cluster centroids.</p>
<p>The formula first calculates the distance between each observation and the cluster centroids where the number of centroids is
equal to the number of clusters that were specified in the cluster analysis in the first set of parentheses.</p>
<pre><code>cdist(clus_train, model.cluster_centers_, &#39;euclidean&#39;)</code></pre>
<p>This tells Python to use cdist to calculate the distance between each observation in the clus_train data set in the cluster centroids using Euclidean distance, then we use np.min function to determine the smallest or minimum difference for each observation among the cluster centroids. Axis equals 1 means that the minimum should be determined by examining the distance between the observation and each centroid taking the smallest distance as the value of the minimum. Then we use the sum function to sum the minimum distances across all observations. Finally, the / clus_train.shape with 0 in brackets divides the sum of the distances by the number of observations in the clus_train data set (.shape[0] returns the number of observations in the clus_train data set).</p>
<p>Now that we have the average distance calculated for each of the 1 to 9 cluster solutions we can plot the elbow curve using the map plot lib plot function that we imported as plt.</p>
<pre><code>
plt.plot(clusters, meandist)
plt.xlabel(&#39;Number of clusters&#39;)
plt.ylabel(&#39;Average distance&#39;)
plt.title(&#39;Selecting k with the Elbow Method&#39;)</code></pre>
<p><img src="./img/01.png" alt=""></p>
<p>Clusters is the object that includes the values of 1 through 9 for the range of clusters we specified and meandist is the average distance value that we just calculated. So what this plot shows is the decrease in the average minimum distance of the observations from the cluster centroids for each of the cluster solutions. We can see that the average distance decreases as the number of clusters increases. Since the goal of cluster analysis is to minimize the distance between observations and their assigned clusters we want to chose the fewest numbers of clusters that provides a low average distance.</p>
<p>What we&#39;re looking for in this plot is a bend in the elbow that kind of shows where the average distance value might be levelling off such that adding more clusters doesn&#39;t decrease the average distance as much. You can see how subjective this is though. There appears to be a couple of bends at the line at two clusters and at three clusters, but it&#39;s not very clear. To help us figure out which of the solutions is best we should further examine the cluster solutions for at least the two and three cluster solutions to see whether they do not overlap, whether the patterns of means on the clustering variables are unique and meaningful, and whether there are significant differences between the clusters on our external validation variable GPA.</p>

          </div>
        
          <div class="ui tab segment lab" data-tab="02">
            <h1>Interpretation</h1>
<p>This time we re run the cluster analysis and ask for 3 clusters.</p>
<p>So we create an object, model 3, which will contain the results from the cluster analysis with 3 clusters.</p>
<pre><code>
model3=KMeans(n_clusters=3)
model3.fit(clus_train)
clusassign=model3.predict(clus_train)</code></pre>
<p>The first thing we want to try is to graph the clusters in a scatter plot to see whether or not they overlap with each other in terms of their location in the p dimensional space. However, with 11 clustering variables that means we have 11 dimensions, which would be impossible to visualize. A scatter plot will work to visualize a few dimensions, but not 11 dimensions. So what we&#39;re going to use is use canonical discriminate analysis, which is a data reduction technique that creates a smaller number of variables that are linear combinations of the 11 clustering variables.</p>
<p>The new variables, called canonical variables, are ordered in terms of the proportion of variance and the clustering variables that is accounted for by each of the canonical variables. So the first canonical variable will count for the largest proportion of the variance. The second canonical variable will account for the next largest proportion of variance, and so on. Usually, the majority of the variance in the clustering variables will be accounted for by the first couple of canonical variables and those are the variables that we can plot. In Python, we can use the PCA function and the sklearn decomposition library to conduct the canonical discriminate analysis.</p>
<pre><code>
from sklearn.decomposition import PCA
pca_2 = PCA(2)
plot_columns = pca_2.fit_transform(clus_train)
plt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=model3.labels_,)
plt.xlabel(&#39;Canonical variable 1&#39;)
plt.ylabel(&#39;Canonical variable 2&#39;)
plt.title(&#39;Scatterplot of Canonical Variables for 3 Clusters&#39;)
plt.show()</code></pre>
<p>Here is the scatter plot. What this shows is that these two clusters are densely packed, meaning that the observations within the clusters are pretty highly correlated with each other, within cluster variance is relatively low. But they appear to have a good deal of overlap, meaning that there is not good separation between these two clusters.</p>
<p><img src="./img/02.png" alt=""></p>
<p>On the other hand, this cluster here shows better separation, but the observations are more spread out indicating less correlation among the observations and higher within cluster variance.</p>
<p><img src="./img/03.png" alt=""></p>
<p>This suggests that the two cluster solution might be better, meaning that it would be especially important to further evaluate the two cluster solution as well.</p>
<p>At this stage there is not sufficient time to go into further depth of analysis in clustering. The continuous assessment will only include up to this point in relation to clustering assessment.</p>

          </div>
        
          <div class="ui tab segment lab" data-tab="03">
            <h1>Cluster analysis continuous assessment</h1>
<p>Download the following file:</p>
<ul>
<li><a href="./archives/gapminder.csv">gapminder data</a></li>
</ul>
<p>Open the file in excel to remind yourself of the data, in particular the column names.</p>
<p>Create a new python file called Lab09_ca.py</p>
<p>Next import in the necessary libraries to run cluster analysis and graph the results.</p>
<p>Then load the data file gapmminder.csv into your data frame.</p>
<p>Conduct the data management you think necessary prior to clustering on the following columns.</p>
<pre><code>INCOMEPERPERSON
FEMALEEMPLOYRATE
INTERNETUSERATE
LIFEEXPECTANCY
ALCCONSUMPTION
URBANRATE</code></pre>
<p>For example, check for empty values and replace with NaN, convert each column to a number.</p>
<p>Next subset the data into a new dataframe.</p>
<p>Next standardize all of the variables.</p>
<p>Then split the data into train and test sets.</p>
<p>Run the cluster analysis using K-means for 1-9 clusters.</p>
<p>Next plot the curve and determine how many is the fewest clusters that provides a low average distance.</p>
<p>Examine the cluster solution for the number of clusters you consider appropriate based on the elbow curve.</p>
<p>What does the scatter plot show us?
Put your answer in the comments in your python file.</p>
<p>Upload your .py files to Moodle:</p>
<ul>
<li>Lab09_clustering</li>
<li>Lab09_ca</li>
</ul>

          </div>
        
      </div>
    </div>
  </div>


  <br>
  <script>
    $(document).on('keydown', function(e) {
  e = e || window.event;
  var nextTab;
  switch (e.which || e.keyCode) {
    case 37: // left
      nextTab = $('.tab-menu a[data-tab].active').prev('a[data-tab]');
      if (!nextTab.length) nextTab = $('.tab-menu a[data-tab]').last();
      nextTab.click();
      $('.pusher').focus();
      break;

    case 39: // right
      nextTab = $('.tab-menu a[data-tab].active').next('a[data-tab]');
      if (!nextTab.length) nextTab = $('.tab-menu a[data-tab]').first();
      nextTab.click();
      $('.pusher').focus();
      break;
  }
});

    $(document).ready(function() {
  $('img').addClass('ui image');

  $('.ui.embed').embed();

  const $images = $('.lab img');
  jQuery.each($images, function(i) {
    if ($images[i].alt.length > 0) {
      const divImg = $(document.createElement('div')).addClass(
        'ui basic segment',
      );
      $($images[i]).wrap(divImg);
      const divLabel = $(document.createElement('div')).addClass(
        'ui blue ribbon label',
      );
      divLabel.append($images[i].alt);
      $(divLabel).insertBefore($images[i]);
    }
  });

  $('.ui.menu .item').tab({
    history: true,
    historyType: 'hash',
  });

  $('.popup').popup();

  $('.ui.sidebar')
    .sidebar({ context: $('.pushable') })
    .sidebar('setting', 'transition', 'slide out')
    .sidebar('attach events', '#toc');
});

  </script>



  <div class="ui bottom fixed borderless right menu">
    <div class="ui right small menu">
      <div class="ui tiny basic message segment">
         Powered by <a href="https://github.com/edeleastar/tutors-ts">tutors-ts.</a> 
      </div>
    </div>
  </div>

  </body>

</html>